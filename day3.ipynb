{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Day 3: Corpus processing with NLTK\n",
    "\n",
    "Na-Rae Han (`naraehan@pitt.edu`) and David J. Birnbaum (`djbpitt@pitt.edu`) \n",
    "\n",
    "June 25-29, [NASSLLI 2018 at CMU](https://www.cmu.edu/nasslli2018/) \n",
    "\n",
    "This tutorial is found on https://github.com/naraehan/NASSLLI2018-Corpus-Linguistics. \n",
    "- Jump to: [Day 1](day1.ipynb), [Day 2](day2.ipynb), [Day 3](day3.ipynb), [Day 4](day4.ipynb), [Day 5](day5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "#### Data\n",
    "\n",
    "- Download and unzip the “C-Span Inaugural Address Corpus”, available on NLTK’s corpora page: http://www.nltk.org/nltk_data/\n",
    "- Place the unzipped `inaugural` folder **on your desktop** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing a  corpus\n",
    "\n",
    "- NLTK can read in an entire corpus from a directory (the “root” directory).\n",
    "- As it reads in a corpus, it applies word tokenization: `.words()` and sentence tokenization: `.sents()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'C:/Users/narae/Desktop/inaugural'  # Use your own userid; Mac users should omit C:\n",
    "inaug = PlaintextCorpusReader(corpus_root, '.*txt')  # all files ending in 'txt' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint  # turn off pretty printing, which prints too many lines\n",
    "# .txt file names as file IDs\n",
    "inaug.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK automatically tokenizes the corpus. First 50 words: \n",
    "inaug.words()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also specify individual file ID. First 50 words from Obama 2009:\n",
    "inaug.words('2009-Obama.txt')[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK automatically segments sentences too, which are accessed through .sents()\n",
    "print(inaug.sents('2009-Obama.txt')[0])   # first sentence\n",
    "print(inaug.sents('2009-Obama.txt')[1])   # 2nd sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long are these speeches in terms of word and sentence count?\n",
    "print('Washington:', len(inaug.words('1789-Washington.txt')), len(inaug.sents('1789-Washington.txt')))\n",
    "print('Obama:', len(inaug.words('2009-Obama.txt')), len(inaug.sents('2009-Obama.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for-loop through file IDs and print out various stats. \n",
    "# While looping, populate fid_avsent which holds avg sent lengths.\n",
    "\n",
    "fid_avsent = []    # initialize an empty list\n",
    "\n",
    "for f in inaug.fileids():\n",
    "    wcount = len(inaug.words(f))\n",
    "    scount = len(inaug.sents(f))\n",
    "    print(wcount, scount, wcount/scount, f, sep='\\t')  # separate by tab for readability\n",
    "    fid_avsent.append( (wcount/scount, f) )      # append a pair (x, y) to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trouble shooting \n",
    "\n",
    "- Unfortunately, 2005 Bush file produces a **Unicode encoding error**. \n",
    "- Let's make a new text file from [http://www.presidency.ucsb.edu/inaugurals.php](http://www.presidency.ucsb.edu/inaugurals.php)\n",
    "- The text files are locked; We will need to save and halt this notebook first. \n",
    "\n",
    "**Mac**:\n",
    "1. Launch TextEdit. It is Mac's default text editor.  \n",
    "1. Visit the web page and copy the text: highlight and `Command+C`. \n",
    "1. Come back to the TextEdit window, paste `Command+V`. \n",
    "1. **Convert to plain text**: `Shift+Command+T`\n",
    "1. Save. Choose the \"inaugural\" directory and give the appropriate file name. Make sure to choose \"**Unicode (UTF-8)**\" as the Encoding. Overwrite the existing file. \n",
    "\n",
    "**Windows**: \n",
    "1. First, delete the offending file. \n",
    "1. Then, right-click empty space in the folder, create a new text file with the same name. \n",
    "1. Double-clicking it will open it in your default text editor (Notepad)\n",
    "1. Visit the web page and copy the text: highlight and `Control+C`. \n",
    "1. Come back to Notepad, paste in (`Control+V`). \n",
    "1. Save: make sure to choose **UTF-8** encoding and **not ANSI**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn pretty print back on \n",
    "%pprint\n",
    "# sorted() returns an alphabetically sorted list\n",
    "sorted(fid_avsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same thing, with list comprehension! \n",
    "fid_avsent2 = [(len(inaug.words(f))/len(inaug.sents(f)), f) for f in inaug.fileids()]\n",
    "sorted(fid_avsent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus size in number of words\n",
    "len(inaug.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building word frequency distribution for the entire corpus\n",
    "inaug_fd = nltk.FreqDist(inaug.words())\n",
    "inaug_fd.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn\n",
    "\n",
    "- Explore the corpus! \n",
    "- Are the following words getting more or less frequent: 'we', 'the'?\n",
    "- Are _words_ getting longer or shorter? Hint: use `sum([1, 2, 3, 4])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More tomorrow\n",
    "\n",
    "- NLTK's other corpus tools: Text, concordancer, ngrams\n",
    "\n",
    "We will learn on [Day 4 (Thursday)](day4.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
