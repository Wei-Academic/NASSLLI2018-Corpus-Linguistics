{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Day 5: Advanced Processing, Bring Your Own Corpora\n",
    "\n",
    "Na-Rae Han (`naraehan@pitt.edu`) and David J. Birnbaum (`djbpitt@pitt.edu`) \n",
    "\n",
    "June 25-29, [NASSLLI 2018 at CMU](https://www.cmu.edu/nasslli2018/) \n",
    "\n",
    "This tutorial is found on https://github.com/naraehan/NASSLLI2018-Corpus-Linguistics. \n",
    "- Jump to: [Day 1](day1.ipynb), [Day 2](day2.ipynb), [Day 3](day3.ipynb), [Day 4](day4.ipynb), [Day 5](day5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced processing: lemmatization\n",
    "- NLTK's WordNet lemmatizer \n",
    "- It works well for nouns. Verbs are tricky: default POS is set to 'noun', and verbs need to be specified as such. \n",
    "- For a better/knowlege-rich/context-aware solution, you might need to venture outside Python/NLTK and try full-scale NLP suites such as [Stanford's Core NLP](https://stanfordnlp.github.io/CoreNLP/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "wnl = nltk.WordNetLemmatizer()   # initialize a lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try 'geese', 'walks', 'walked', 'walking' \n",
    "wnl.lemmatize('cats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl.lemmatize('walking', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From this page: http://www.pitt.edu/~naraehan/python3/text-samples.txt\n",
    "moby = \"\"\"Call me Ishmael. Some years ago--never mind how long precisely--having\n",
    "little or no money in my purse, and nothing particular to interest me on\n",
    "shore, I thought I would sail about a little and see the watery part of\n",
    "the world. It is a way I have of driving off the spleen and regulating\n",
    "the circulation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pprint\n",
    "nltk.word_tokenize(moby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[wnl.lemmatize(t) for t in nltk.word_tokenize(moby)]\n",
    "# Output isn't very intelligent without us supplying individual tokens with their correct POS \n",
    "# Any way to identify verbs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced processing: POS tagging\n",
    "- `nltk.pos_tag` is NLTK's default POS tagger.  \n",
    "- Default tagset is the [Penn Treebank ('wsj') tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html). \n",
    "- A word of warning: it is not state-of-the-art. (Built on limited data.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chom = 'colorless green ideas sleep furiously'.split()\n",
    "chom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(chom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(nltk.word_tokenize(moby))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(nltk.pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring Your Own Corpora (1): Treebanks\n",
    "- Treebanks are syntactically annotated sentences. \n",
    "- They are used in training POS-taggers and syntactic parsers. \n",
    "- NLTK includes a sample section of the Penn English Treebank (3914 sentences and about 10% of the entire corpus). \n",
    "- For more details on Treebanks and how to interact with tree structure, see [this NLTK book section](http://www.nltk.org/book/ch08.html#treebanks-and-grammars). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "treebank.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treebank.parsed_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note: just flashing the first tree will give you an \"unable to find the gs file\" error. \n",
    "#    Saving it into t works, however. \n",
    "# https://stackoverflow.com/questions/36942270/nltk-was-unable-to-find-the-gs-file/37160385\n",
    "# In short: you need to install GhostScript and add it to your system's PATH. \n",
    "\n",
    "t = treebank.parsed_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trees are composed of subtrees, each of which itself is a Tree. \n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Opens up a new window. Close it before moving to next cell. \n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"said\" is a verb (VBD) that takes a clausal complement (S). \n",
    "#   The nodes are children of a VP node. \n",
    "print(treebank.parsed_sents()[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# myfilter: returns True/False on whether current Tree is a VP node with an S child. \n",
    "# You can define your own function through def keyword. \n",
    "\n",
    "def myfilter(tree):\n",
    "    child_nodes = [child.label() for child in tree if isinstance(child, nltk.Tree)]\n",
    "    return  (tree.label() == 'VP') and ('S' in child_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every full tree in the Treebank, recurse through its subtrees, \n",
    "#    filter in only those that meet the configuration. \n",
    "# Searching through first 50 sentences only: remove [:50] for a full search. \n",
    "\n",
    "%pprint \n",
    "[subtree for tree in treebank.parsed_sents()[:50]\n",
    "             for subtree in tree.subtrees(myfilter)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treebanks in Non-English\n",
    "- A sample of 'Sinica Treebank' (Chinese) is available as part of NLTK's data. \n",
    "- You should download it first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('sinica_treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sinica_treebank as chtb\n",
    "print(chtb.parsed_sents()[3450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chtb.parsed_sents()[3450].draw()    # Opens a new window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring Your Own Corpora (2): CHILDES\n",
    "**CHAT vs. XML**\n",
    "- CHILDES uses its own corpus format: CHAT. \n",
    "- Many data sets also come in XML format (https://childes.talkbank.org/data-xml/), which NLTK can read in.\n",
    "- If no XML version is provided, you can use a converter called Chatter: https://talkbank.org/software/chatter.html\n",
    "\n",
    "**Getting the data**\n",
    "1. Navigate to <https://childes.talkbank.org/data-xml/>.\n",
    "1. Click on the link to the language that interests you, e.g., `Eng-NA` (North American English). These directories hold `zip` archives of subcorpora in the designated language.\n",
    "1. Download one of more of the zip files, say `Valian.zip`. \n",
    "1. Create a new directory named `CHILDES` on your Desktop. Unzip the downloaded file into it. \n",
    "1. Now you should have a `Valian` directory inside `CHILDES`. \n",
    "\n",
    "**Starter code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader import CHILDESCorpusReader\n",
    "corpus_root = 'C:/Users/narae/Desktop/CHILDES/Valian' # change path as needed\n",
    "valian = CHILDESCorpusReader(corpus_root, '.*.xml')\n",
    "\n",
    "valian.fileids()         # returns list of filenames\n",
    "len(valian.fileids())    # returns count of files in corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If that all works, navigate to <http://www.nltk.org/howto/childes.html> and begin at the line that reads “Printing properties of the corpus files”.\n",
    "- More CHILDES & Python tutorials:\n",
    "  - http://ling-blogs.bu.edu/lx390f17/standoff-annotation-xml-and-more-childes/\n",
    "  - http://aaronstevenwhite.io/language-acquisition/working-with-childes-part1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about...?\n",
    "- Files in MS Word or PDF? (See [this NLTK book section](http://www.nltk.org/book/ch03.html#extracting-text-from-pdf-msword-and-other-binary-formats))\n",
    "- Non-English corpora? (See [this NLTK book section](http://www.nltk.org/book/ch02.html#corpora-in-other-languages))\n",
    "- Corpora in XML format? (See [this NLTK book section](http://www.nltk.org/book/ch11.html#working-with-xml))\n",
    "- Looking to load your own annotated corpus (POS-tagged, Treebanks, etc.)? NLTK provides specialized corpus loaders for such formats: see [this NLTK how-to page](http://www.nltk.org/howto/corpus.html).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "Take a Python course! There are many online courses available on [Coursera](http://www.coursera.org), [EdX](https://www.edx.org/), [udemy](https://www.udemy.com/courses/), [DataCamp](https://www.datacamp.com/courses), and more.\n",
    "\n",
    "The NLTK book \"Natural Language Processing with Python\" is available here: http://www.nltk.org/book/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
